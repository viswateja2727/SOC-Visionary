# 📚 Weekly Reading Plan – Week 3: Training Neural Networks & Getting Started with PyTorch

This week, we combine **core deep learning training concepts** with hands-on learning in **PyTorch**. You’ll learn how models actually improve during training using **loss functions**, **backpropagation**, and **gradient descent**—and you’ll start implementing these ideas using PyTorch.

---

## 🧠 Goals for the Week

- Understand **loss functions**, **gradient descent**, and **backpropagation**
- Learn about optimizers like **SGD**, **Adam**, and how they affect training
- Get hands-on with PyTorch: tensors, autograd, and basic neural networks
- Train a simple neural network on real data (like MNIST)

---

## 📌 Theory Resources

1. 📝 **Blog Post:** [Loss Functions and Optimization in Neural Networks](https://towardsdatascience.com/understanding-loss-functions-in-neural-networks-1c0f6a86d72)  
   *Explains common loss functions and their role in learning.*

2. 📝 **Blog Post:** [Gradient Descent Explained](https://towardsdatascience.com/gradient-descent-explained-9b953fc0d2ec)  
   *Covers the mathematical intuition and variations of gradient descent.*

3. 🎥 **Video:** [Backpropagation Explained Visually](https://www.youtube.com/watch?v=Ilg3gGewQ5U)  
   *Clear visual walkthrough of how backpropagation works.*

---

## 💻 PyTorch Practice Resources

1. 🎥 **Beginner-Friendly Course:** [PyTorch for Beginners – YouTube Playlist](https://www.youtube.com/playlist?list=PL_lsbAsL_o2CTlGHgMxNrKhzP97BaG9ZN)  
   *Start here for hands-on understanding of PyTorch basics.*

2. 🎥 **Hands-on Deep Learning with PyTorch (First 9 videos)**:  
   [Deep Learning with PyTorch – Playlist](https://www.youtube.com/playlist?list=PLCC34OHNcOtpcgR9LEYSdi9r7XIbpkpK1)  
   *Focus on the first 9 videos to build strong foundations.*

---

⏳ *Target completion: 3–5 days to stay on track with your weekly pace.*

🧠 *Pro Tip: Recreate simple examples from the videos/blogs in a Jupyter notebook to reinforce learning!*

Happy learning! 🚀
